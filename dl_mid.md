# Deep Learning Mid Term Examination Solutions

## SRM UNIVERSITY â€“ AP, ANDHRA PRADESH

**Subject:** Deep Learning (CSE 457)  
**Examination:** Mid Term, October 2025  
**Student Name:** _[Your Name]_  
**Roll Number:** _[Your Roll Number]_  
**Max Marks:** 25 | **Duration:** 1 hour

---

# PART A (2 Ã— 10 Marks = 20 Marks)

## Question 1. (A)

**Design a simple perceptron to simulate the logical OR gate. Use the initial weights: wâ‚ = 0.9, wâ‚‚ = 0.6, Bias b = 0.1. Train the perceptron using the gradient descent algorithm with learning rate lr = 0.5. (Note: Consider cost function as MSE.)**

### Solution:

#### **Step 1: OR Gate Truth Table**

| xâ‚  | xâ‚‚  | Target (y) |
| --- | --- | ---------- |
| 0   | 0   | 0          |
| 0   | 1   | 1          |
| 1   | 0   | 1          |
| 1   | 1   | 1          |

#### **Step 2: Perceptron Model**

The perceptron output is calculated as:

```
net = wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚ + b
Å· = activation(net)
```

For simplicity, we'll use a **step activation function**:

```
activation(net) = 1 if net â‰¥ 0.5, else 0
```

#### **Step 3: Initial Parameters**

- wâ‚ = 0.9
- wâ‚‚ = 0.6
- b = 0.1
- Learning rate (lr) = 0.5
- Loss function: MSE = (1/n) Ã— Î£(y - Å·)Â²

#### **Step 4: Training Process (Gradient Descent)**

**Iteration 1: Input (0, 0), Target = 0**

1. **Forward Pass:**
   - net = 0.9(0) + 0.6(0) + 0.1 = 0.1
   - Å· = 0 (since 0.1 < 0.5)
2. **Error Calculation:**
   - Error (e) = y - Å· = 0 - 0 = 0
   - MSE = (0)Â² = 0
3. **Weight Update:**
   - Since error = 0, no update needed
   - wâ‚ = 0.9, wâ‚‚ = 0.6, b = 0.1

---

**Iteration 2: Input (0, 1), Target = 1**

1. **Forward Pass:**
   - net = 0.9(0) + 0.6(1) + 0.1 = 0.7
   - Å· = 1 (since 0.7 â‰¥ 0.5)
2. **Error Calculation:**
   - Error (e) = y - Å· = 1 - 1 = 0
   - MSE = (0)Â² = 0
3. **Weight Update:**
   - Since error = 0, no update needed
   - wâ‚ = 0.9, wâ‚‚ = 0.6, b = 0.1

---

**Iteration 3: Input (1, 0), Target = 1**

1. **Forward Pass:**
   - net = 0.9(1) + 0.6(0) + 0.1 = 1.0
   - Å· = 1 (since 1.0 â‰¥ 0.5)
2. **Error Calculation:**
   - Error (e) = y - Å· = 1 - 1 = 0
   - MSE = (0)Â² = 0
3. **Weight Update:**
   - Since error = 0, no update needed
   - wâ‚ = 0.9, wâ‚‚ = 0.6, b = 0.1

---

**Iteration 4: Input (1, 1), Target = 1**

1. **Forward Pass:**
   - net = 0.9(1) + 0.6(1) + 0.1 = 1.6
   - Å· = 1 (since 1.6 â‰¥ 0.5)
2. **Error Calculation:**
   - Error (e) = y - Å· = 1 - 1 = 0
   - MSE = (0)Â² = 0
3. **Weight Update:**
   - Since error = 0, no update needed
   - wâ‚ = 0.9, wâ‚‚ = 0.6, b = 0.1

#### **Step 5: Gradient Descent Update Rule**

For MSE loss, the gradient descent update rules are:

```
âˆ‚MSE/âˆ‚wâ‚ = -2(y - Å·)Â·xâ‚
âˆ‚MSE/âˆ‚wâ‚‚ = -2(y - Å·)Â·xâ‚‚
âˆ‚MSE/âˆ‚b = -2(y - Å·)

wâ‚_new = wâ‚ - lr Ã— âˆ‚MSE/âˆ‚wâ‚ = wâ‚ + 2Â·lrÂ·(y - Å·)Â·xâ‚
wâ‚‚_new = wâ‚‚ - lr Ã— âˆ‚MSE/âˆ‚wâ‚‚ = wâ‚‚ + 2Â·lrÂ·(y - Å·)Â·xâ‚‚
b_new = b - lr Ã— âˆ‚MSE/âˆ‚b = b + 2Â·lrÂ·(y - Å·)
```

#### **Step 6: Verification**

After training, let's verify all inputs:

| Input (xâ‚, xâ‚‚) | net value | Output (Å·) | Target (y) | Correct? |
| -------------- | --------- | ---------- | ---------- | -------- |
| (0, 0)         | 0.1       | 0          | 0          | âœ“        |
| (0, 1)         | 0.7       | 1          | 1          | âœ“        |
| (1, 0)         | 1.0       | 1          | 1          | âœ“        |
| (1, 1)         | 1.6       | 1          | 1          | âœ“        |

#### **Conclusion:**

The initial weights (wâ‚ = 0.9, wâ‚‚ = 0.6, b = 0.1) already correctly implement the OR gate logic. The perceptron successfully classifies all four input combinations without requiring weight updates. The final weights remain:

- **wâ‚ = 0.9**
- **wâ‚‚ = 0.6**
- **b = 0.1**

The OR gate is successfully simulated as the perceptron outputs 1 when at least one input is 1, and outputs 0 only when both inputs are 0.

---

## Question 1. (B)

**Differentiate the classification and regression models and explain any two feasible loss functions in each case.**

### Solution:

#### **1. Difference Between Classification and Regression**

| Aspect                 | Classification                                       | Regression                                                   |
| ---------------------- | ---------------------------------------------------- | ------------------------------------------------------------ |
| **Output Type**        | Discrete/Categorical (Classes)                       | Continuous Numerical Values                                  |
| **Goal**               | Predict category/class labels                        | Predict numerical quantities                                 |
| **Output Range**       | Finite set of classes                                | Infinite possible values                                     |
| **Example Tasks**      | Spam detection, image recognition, disease diagnosis | House price prediction, temperature forecasting, stock price |
| **Evaluation Metrics** | Accuracy, Precision, Recall, F1-Score, AUC-ROC       | MSE, RMSE, MAE, RÂ² Score                                     |
| **Output Activation**  | Softmax, Sigmoid                                     | Linear, ReLU                                                 |
| **Example Output**     | "Cat", "Dog", "Bird" or 0, 1, 2                      | 45.7, 123.45, -15.2                                          |

#### **2. Loss Functions for Classification**

##### **A) Binary Cross-Entropy Loss (Log Loss)**

**Used for:** Binary classification problems (two classes: 0 or 1)

**Mathematical Formula:**

```
L = -1/n Ã— Î£[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]
```

Where:

- y = actual label (0 or 1)
- Å· = predicted probability
- n = number of samples

**Properties:**

- Output range: [0, âˆ)
- Penalizes confident wrong predictions heavily
- Works well with sigmoid activation
- Convex function (easy to optimize)

**Example:**

```
Actual: y = 1 (positive class)
Predicted: Å· = 0.8 (80% confidence)
Loss = -(1Â·log(0.8) + 0Â·log(0.2)) = -log(0.8) = 0.223

Predicted: Å· = 0.2 (wrong prediction)
Loss = -(1Â·log(0.2)) = 1.609 (much higher penalty)
```

**Use Case:** Email spam detection, fraud detection, medical diagnosis (disease present/absent)

---

##### **B) Categorical Cross-Entropy Loss**

**Used for:** Multi-class classification problems (more than 2 classes)

**Mathematical Formula:**

```
L = -1/n Ã— Î£Î£ y_ic Â· log(Å·_ic)
```

Where:

- y_ic = 1 if sample i belongs to class c, else 0 (one-hot encoded)
- Å·_ic = predicted probability for class c
- n = number of samples
- C = number of classes

**Properties:**

- Extension of binary cross-entropy for multiple classes
- Uses one-hot encoding for true labels
- Works with softmax activation function
- Encourages the model to output high probability for correct class

**Example:**

```
3 classes: Cat, Dog, Bird
Actual: [1, 0, 0] (Cat)
Predicted: [0.7, 0.2, 0.1]
Loss = -(1Â·log(0.7) + 0Â·log(0.2) + 0Â·log(0.1)) = -log(0.7) = 0.357

Predicted: [0.2, 0.6, 0.2] (wrong)
Loss = -log(0.2) = 1.609 (higher penalty)
```

**Use Case:** Image classification (MNIST, CIFAR-10), document categorization, sentiment analysis

---

#### **3. Loss Functions for Regression**

##### **A) Mean Squared Error (MSE) / L2 Loss**

**Used for:** Most regression problems, especially when large errors are undesirable

**Mathematical Formula:**

```
MSE = 1/n Ã— Î£(y - Å·)Â²
```

Where:

- y = actual value
- Å· = predicted value
- n = number of samples

**Properties:**

- Squares the errors (always positive)
- Heavily penalizes large errors (quadratic penalty)
- Sensitive to outliers
- Differentiable everywhere (smooth gradients)
- Assumes Gaussian error distribution

**Example:**

```
Actual: [100, 200, 150]
Predicted: [110, 190, 160]
Errors: [10, -10, 10]
MSE = (10Â² + (-10)Â² + 10Â²) / 3 = (100 + 100 + 100) / 3 = 100

With outlier:
Actual: [100, 200, 150, 1000]
Predicted: [110, 190, 160, 200]
Error for outlier: (1000-200)Â² = 640,000 (dominates the loss!)
```

**Advantages:**

- Smooth optimization landscape
- Well-suited for normally distributed errors
- Commonly used and well-understood

**Disadvantages:**

- Sensitive to outliers
- Units are squared (less interpretable)

**Use Case:** Temperature prediction, stock price forecasting, sensor calibration

---

##### **B) Mean Absolute Error (MAE) / L1 Loss**

**Used for:** Regression problems with outliers or when all errors should be weighted equally

**Mathematical Formula:**

```
MAE = 1/n Ã— Î£|y - Å·|
```

Where:

- y = actual value
- Å· = predicted value
- n = number of samples

**Properties:**

- Takes absolute value of errors
- Linear penalty for errors
- More robust to outliers than MSE
- Less smooth at zero (gradient is constant)
- Assumes Laplacian error distribution

**Example:**

```
Actual: [100, 200, 150]
Predicted: [110, 190, 160]
Errors: [10, -10, 10]
MAE = (|10| + |-10| + |10|) / 3 = 30 / 3 = 10

With outlier:
Actual: [100, 200, 150, 1000]
Predicted: [110, 190, 160, 200]
Error for outlier: |1000-200| = 800
MAE = (10 + 10 + 10 + 800) / 4 = 207.5 (less dominated by outlier)
```

**Advantages:**

- Robust to outliers
- Same units as the target variable (interpretable)
- Treats all errors equally

**Disadvantages:**

- Non-differentiable at zero
- Gradient doesn't decrease as we approach minimum
- Can be slower to converge

**Use Case:** House price prediction (with luxury outliers), delivery time estimation, revenue forecasting

---

#### **4. Comparison Summary**

**Classification Loss Functions:**

- Focus on probability distributions and class predictions
- Non-linear (logarithmic) to handle probabilities [0,1]
- Paired with sigmoid/softmax activations
- Penalize confidence in wrong predictions

**Regression Loss Functions:**

- Focus on minimizing distance between predicted and actual values
- Linear (MAE) or quadratic (MSE) penalties
- Paired with linear/ReLU activations
- Different sensitivity to outliers

---

## Question 2. (A)

**Consider the below feedforward neural network with inputs: a, b, c, d; Hidden layer: 2 neurons (u, v) with ReLU activation; Output layer: 1 neuron (x) with sigmoid activation.**

**Weights: w(aâ†’u)=1, w(bâ†’u)=2, w(câ†’v)=1, w(dâ†’v)=2, w(uâ†’x)=15, w(vâ†’x)=15**

**(i) Perform the forward pass and compute the final network output Å· for input values [a, b, c, d] = [2, 3, 2, 4].**

**(ii) For input values [a, b, c, d] = [2, 3, 2, 4] and target value y = 0, apply one iteration of backpropagation using gradient descent and compute the updated weights.**

### Solution:

#### **Network Architecture:**

```
Input Layer:        Hidden Layer:      Output Layer:
                    (ReLU)             (Sigmoid)
  a(2) â”€â”€â”€â”€â”€1â”€â”€â”€â”€â”€â”
                   u â”€â”€â”€â”€â”€15â”€â”€â”€â”€â”
  b(3) â”€â”€â”€â”€â”€2â”€â”€â”€â”€â”€â”˜             â”‚
                                 x (Å·)
  c(2) â”€â”€â”€â”€â”€1â”€â”€â”€â”€â”€â”             â”‚
                   v â”€â”€â”€â”€â”€15â”€â”€â”€â”€â”˜
  d(4) â”€â”€â”€â”€â”€2â”€â”€â”€â”€â”€â”˜
```

---

### **(i) Forward Pass**

#### **Step 1: Calculate Hidden Layer Neuron u**

Weighted sum at u:

```
z_u = w(aâ†’u)Â·a + w(bâ†’u)Â·b
z_u = 1Â·(2) + 2Â·(3)
z_u = 2 + 6 = 8
```

Apply ReLU activation:

```
ReLU(z) = max(0, z)
u = ReLU(8) = max(0, 8) = 8
```

#### **Step 2: Calculate Hidden Layer Neuron v**

Weighted sum at v:

```
z_v = w(câ†’v)Â·c + w(dâ†’v)Â·d
z_v = 1Â·(2) + 2Â·(4)
z_v = 2 + 8 = 10
```

Apply ReLU activation:

```
v = ReLU(10) = max(0, 10) = 10
```

#### **Step 3: Calculate Output Layer Neuron x**

Weighted sum at x:

```
z_x = w(uâ†’x)Â·u + w(vâ†’x)Â·v
z_x = 15Â·(8) + 15Â·(10)
z_x = 120 + 150 = 270
```

Apply Sigmoid activation:

```
Ïƒ(z) = 1 / (1 + e^(-z))
Å· = Ïƒ(270) = 1 / (1 + e^(-270))
Å· â‰ˆ 1 / (1 + 0) â‰ˆ 1.0
```

**Note:** e^(-270) is extremely small (â‰ˆ 10^(-117)), so sigmoid(270) â‰ˆ 1.0

#### **Forward Pass Summary:**

```
Inputs: a=2, b=3, c=2, d=4
Hidden layer: u=8, v=10
Output: Å· â‰ˆ 1.0
```

**Final Network Output: Å· â‰ˆ 1.0**

---

### **(ii) Backpropagation and Weight Update**

Given:

- Input: [a, b, c, d] = [2, 3, 2, 4]
- Target: y = 0
- Predicted: Å· â‰ˆ 1.0
- Learning rate: lr (assume lr = 0.01 for practical computation)

#### **Step 1: Calculate Output Layer Error**

Using Binary Cross-Entropy Loss derivative:

```
L = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]

âˆ‚L/âˆ‚Å· = -(y/Å· - (1-y)/(1-Å·))
âˆ‚L/âˆ‚Å· = -(0/1 - 1/(1-1))

For numerical stability, let's use Å· = 0.9999999
âˆ‚L/âˆ‚Å· â‰ˆ -(-1/0.0000001) â‰ˆ 10,000,000 (very large!)
```

For sigmoid output with cross-entropy, we can use simplified formula:

```
Î´_x = Å· - y = 1.0 - 0 = 1.0
```

#### **Step 2: Gradients for Output Layer Weights**

Gradient for w(uâ†’x):

```
âˆ‚L/âˆ‚w(uâ†’x) = Î´_x Â· u = 1.0 Â· 8 = 8.0
```

Gradient for w(vâ†’x):

```
âˆ‚L/âˆ‚w(vâ†’x) = Î´_x Â· v = 1.0 Â· 10 = 10.0
```

#### **Step 3: Backpropagate Error to Hidden Layer**

Error at neuron u:

```
Î´_u = Î´_x Â· w(uâ†’x) Â· ReLU'(z_u)
```

ReLU derivative:

```
ReLU'(z) = 1 if z > 0, else 0
ReLU'(8) = 1
```

Therefore:

```
Î´_u = 1.0 Â· 15 Â· 1 = 15.0
```

Error at neuron v:

```
Î´_v = Î´_x Â· w(vâ†’x) Â· ReLU'(z_v)
ReLU'(10) = 1
Î´_v = 1.0 Â· 15 Â· 1 = 15.0
```

#### **Step 4: Gradients for Hidden Layer Weights**

Gradient for w(aâ†’u):

```
âˆ‚L/âˆ‚w(aâ†’u) = Î´_u Â· a = 15.0 Â· 2 = 30.0
```

Gradient for w(bâ†’u):

```
âˆ‚L/âˆ‚w(bâ†’u) = Î´_u Â· b = 15.0 Â· 3 = 45.0
```

Gradient for w(câ†’v):

```
âˆ‚L/âˆ‚w(câ†’v) = Î´_v Â· c = 15.0 Â· 2 = 30.0
```

Gradient for w(dâ†’v):

```
âˆ‚L/âˆ‚w(dâ†’v) = Î´_v Â· d = 15.0 Â· 4 = 60.0
```

#### **Step 5: Weight Updates (Gradient Descent)**

Assuming learning rate lr = 0.01:

**Output Layer Weights:**

```
w(uâ†’x)_new = w(uâ†’x) - lr Â· âˆ‚L/âˆ‚w(uâ†’x)
w(uâ†’x)_new = 15 - 0.01 Â· 8.0 = 15 - 0.08 = 14.92

w(vâ†’x)_new = w(vâ†’x) - lr Â· âˆ‚L/âˆ‚w(vâ†’x)
w(vâ†’x)_new = 15 - 0.01 Â· 10.0 = 15 - 0.10 = 14.90
```

**Hidden Layer Weights:**

```
w(aâ†’u)_new = 1 - 0.01 Â· 30.0 = 1 - 0.30 = 0.70

w(bâ†’u)_new = 2 - 0.01 Â· 45.0 = 2 - 0.45 = 1.55

w(câ†’v)_new = 1 - 0.01 Â· 30.0 = 1 - 0.30 = 0.70

w(dâ†’v)_new = 2 - 0.01 Â· 60.0 = 2 - 0.60 = 1.40
```

#### **Summary of Updated Weights:**

| Weight | Initial Value | Gradient | Updated Value |
| ------ | ------------- | -------- | ------------- |
| w(aâ†’u) | 1.0           | 30.0     | 0.70          |
| w(bâ†’u) | 2.0           | 45.0     | 1.55          |
| w(câ†’v) | 1.0           | 30.0     | 0.70          |
| w(dâ†’v) | 2.0           | 60.0     | 1.40          |
| w(uâ†’x) | 15.0          | 8.0      | 14.92         |
| w(vâ†’x) | 15.0          | 10.0     | 14.90         |

#### **Interpretation:**

All weights decreased because:

1. The network predicted Å· â‰ˆ 1.0 but the target was y = 0
2. The large error (1.0) propagated through the network
3. All weights contributed to this overestimation
4. Gradient descent reduced all weights to decrease the output in future iterations

---

## Question 2. (B)

**Explain the working principles of Stochastic Gradient Descent (SGD) and Adam optimizers. How does Adam improve upon the limitations of SGD?**

### Solution:

#### **1. Stochastic Gradient Descent (SGD)**

##### **Working Principle:**

SGD is an optimization algorithm that updates model weights by computing gradients on a **single sample** or a **small batch** of samples at a time, rather than the entire dataset.

**Algorithm:**

```
For each epoch:
    Shuffle training data
    For each sample (or mini-batch):
        1. Compute loss: L = loss_function(y_true, y_pred)
        2. Compute gradient: g = âˆ‚L/âˆ‚w
        3. Update weights: w = w - lr Â· g
```

**Mathematical Update Rule:**

```
w_t+1 = w_t - Î· Â· âˆ‡L(w_t; x_i, y_i)
```

Where:

- w_t = weights at iteration t
- Î· = learning rate (fixed)
- âˆ‡L = gradient of loss
- (x_i, y_i) = single training sample

##### **Variants:**

1. **Vanilla SGD:** Updates on single samples
2. **Mini-batch SGD:** Updates on small batches (most common)
3. **Batch GD:** Updates on entire dataset (deterministic)

##### **Characteristics:**

**Advantages:**

- âœ“ Fast updates (doesn't wait for full dataset)
- âœ“ Can escape local minima due to noisy updates
- âœ“ Memory efficient (processes small batches)
- âœ“ Enables online learning
- âœ“ Works well with large datasets

**Limitations:**

- âœ— Noisy convergence path (oscillations)
- âœ— Fixed learning rate (no adaptation)
- âœ— Same learning rate for all parameters
- âœ— Struggles with ravines (steep in one dimension, gentle in others)
- âœ— Difficult to choose optimal learning rate
- âœ— Can get stuck in saddle points
- âœ— Slow convergence near minimum

##### **Example:**

```
Dataset: 1000 samples
Batch size: 32

Batch GD: 1 update per epoch (1000 samples)
SGD: 31 updates per epoch (32 samples each)
â†’ SGD converges ~31Ã— faster per epoch!
```

---

#### **2. Adam Optimizer (Adaptive Moment Estimation)**

##### **Working Principle:**

Adam combines the best properties of:

- **AdaGrad:** Adapts learning rates for each parameter
- **RMSProp:** Uses moving average of squared gradients
- **Momentum:** Accumulates velocity in consistent directions

Adam maintains **two moving averages** for each parameter:

1. **First moment (m):** Mean of gradients (momentum)
2. **Second moment (v):** Mean of squared gradients (adaptive learning rate)

**Algorithm:**

```
Initialize:
    m_0 = 0 (first moment vector)
    v_0 = 0 (second moment vector)
    t = 0 (timestep)

For each iteration:
    t = t + 1

    1. Compute gradient: g_t = âˆ‡L(w_t)

    2. Update biased first moment:
       m_t = Î²â‚ Â· m_t-1 + (1 - Î²â‚) Â· g_t

    3. Update biased second moment:
       v_t = Î²â‚‚ Â· v_t-1 + (1 - Î²â‚‚) Â· g_tÂ²

    4. Compute bias-corrected first moment:
       mÌ‚_t = m_t / (1 - Î²â‚^t)

    5. Compute bias-corrected second moment:
       vÌ‚_t = v_t / (1 - Î²â‚‚^t)

    6. Update parameters:
       w_t+1 = w_t - Î± Â· mÌ‚_t / (âˆšvÌ‚_t + Îµ)
```

**Hyperparameters (typical values):**

- Î± = 0.001 (learning rate)
- Î²â‚ = 0.9 (exponential decay rate for first moment)
- Î²â‚‚ = 0.999 (exponential decay rate for second moment)
- Îµ = 10â»â¸ (small constant for numerical stability)

##### **Key Components:**

1. **Momentum (m_t):**

   - Accumulates gradient direction
   - Helps accelerate in consistent directions
   - Reduces oscillations

2. **Adaptive Learning Rate (v_t):**

   - Tracks magnitude of recent gradients
   - Larger for parameters with large gradients
   - Smaller for parameters with small gradients

3. **Bias Correction:**
   - Corrects initialization bias (mâ‚€ = 0, vâ‚€ = 0)
   - Important in early training steps
   - Ensures unbiased estimates

##### **Characteristics:**

**Advantages:**

- âœ“ Adapts learning rate for each parameter individually
- âœ“ Handles sparse gradients well
- âœ“ Robust to hyperparameter choice
- âœ“ Combines benefits of momentum and adaptive learning
- âœ“ Works well with noisy/sparse data
- âœ“ Efficient memory usage
- âœ“ Generally faster convergence
- âœ“ Less sensitive to initial learning rate

**Limitations:**

- âœ— More hyperparameters to tune
- âœ— Slightly more computation per update
- âœ— May converge to different solutions than SGD
- âœ— Can sometimes generalize worse than SGD with tuned LR

---

#### **3. How Adam Improves Upon SGD Limitations**

| SGD Limitation                 | How Adam Addresses It                                                           |
| ------------------------------ | ------------------------------------------------------------------------------- |
| **Fixed learning rate**        | Adam adapts learning rate per parameter based on gradient history               |
| **Same LR for all parameters** | Each parameter gets individual learning rate via v_t                            |
| **Noisy convergence**          | Momentum (m_t) smooths gradient updates, reducing oscillations                  |
| **Slow in ravines**            | Momentum accelerates movement in consistent gradient directions                 |
| **Stuck in saddle points**     | Adaptive learning rates help escape flat regions faster                         |
| **Manual LR tuning needed**    | Default hyperparameters (Î±=0.001, Î²â‚=0.9, Î²â‚‚=0.999) work well for most problems |
| **Slow convergence**           | Combination of momentum and adaptive LR speeds up convergence                   |

##### **Detailed Improvements:**

**1. Adaptive Learning Rates:**

SGD:

```
w = w - 0.01 Â· g  (same 0.01 for all parameters)
```

Adam:

```
w = w - 0.001 Â· mÌ‚ / (âˆšvÌ‚ + Îµ)
Effective LR varies: 0.001, 0.0005, 0.002, etc.
(different for each parameter based on gradient history)
```

**2. Handling Sparse Features:**

- **SGD:** Parameters with rare gradients update slowly
- **Adam:** Larger effective learning rate for sparse parameters (smaller v_t)
- **Impact:** Better for NLP, recommendation systems with sparse inputs

**3. Convergence Behavior:**

```
Visualization:

SGD Path:        Adam Path:
    â†˜              â†˜
     â†˜            â†’
    â†™             â†“
   â†™              â†“
  â†˜               ğŸ¯
 â†™                (smooth, direct)
ğŸ¯
(zigzag, slower)
```

**4. Example Scenario:**

Consider training on loss surface with different curvatures:

```
Parameter wâ‚: steep gradient (âˆ‚L/âˆ‚wâ‚ = -100)
Parameter wâ‚‚: gentle gradient (âˆ‚L/âˆ‚wâ‚‚ = -0.01)

SGD (lr = 0.01):
wâ‚ = wâ‚ - 0.01 Â· (-100) = wâ‚ + 1.0  (might overshoot!)
wâ‚‚ = wâ‚‚ - 0.01 Â· (-0.01) = wâ‚‚ + 0.0001  (too slow!)

Adam:
Adapts: larger step for wâ‚‚, smaller step for wâ‚
wâ‚: effective_lr â‰ˆ 0.001 (reduced due to large vâ‚)
wâ‚‚: effective_lr â‰ˆ 0.01 (increased due to small vâ‚‚)
â†’ Balanced, efficient updates
```

##### **5. Practical Comparison:**

| Aspect                         | SGD                                             | Adam                                   |
| ------------------------------ | ----------------------------------------------- | -------------------------------------- |
| **Best for**                   | Small datasets, well-tuned scenarios            | Large datasets, default starting point |
| **Convergence speed**          | Slower                                          | Faster (typically)                     |
| **Hyperparameter sensitivity** | High (requires careful LR tuning)               | Low (robust defaults)                  |
| **Generalization**             | Often better (with good tuning)                 | Good (but sometimes overfits)          |
| **Memory overhead**            | Minimal                                         | 2Ã— (stores m and v)                    |
| **Computation per step**       | Lowest                                          | Slightly higher                        |
| **Use when**                   | You have time to tune, need best generalization | Quick experimentation, large scale     |

##### **6. When to Use Each:**

**Use SGD when:**

- You have time for extensive hyperparameter tuning
- Working with small to medium datasets
- Need best possible generalization
- Training convolutional networks (often works well)
- You have learning rate schedule expertise

**Use Adam when:**

- Starting a new project (good default choice)
- Working with large datasets
- Training RNNs or transformers
- Need fast convergence
- Limited time for hyperparameter tuning
- Working with sparse data

---

#### **Conclusion:**

Adam represents a significant advancement over SGD by:

1. **Automating learning rate adaptation**
2. **Combining momentum for acceleration**
3. **Handling diverse gradient magnitudes**
4. **Providing robust default hyperparameters**

However, SGD with proper tuning (especially with learning rate schedules and momentum) can still achieve superior generalization in some scenarios. The choice depends on the specific problem, dataset size, and available tuning time.

---

# PART B (5 Ã— 1 Marks = 5 Marks)

## Question 3

**Draw the Venn diagram to represent the relation among AI, ML, and DL.**

### Solution:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Artificial Intelligence (AI)                â”‚
â”‚  (Machines mimicking human intelligence)            â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚    Machine Learning (ML)                 â”‚      â”‚
â”‚  â”‚  (Learning from data without explicit    â”‚      â”‚
â”‚  â”‚   programming)                           â”‚      â”‚
â”‚  â”‚                                          â”‚      â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚      â”‚
â”‚  â”‚    â”‚   Deep Learning (DL)        â”‚      â”‚      â”‚
â”‚  â”‚    â”‚  (Neural networks with      â”‚      â”‚      â”‚
â”‚  â”‚    â”‚   multiple layers)          â”‚      â”‚      â”‚
â”‚  â”‚    â”‚                             â”‚      â”‚      â”‚
â”‚  â”‚    â”‚  â€¢ CNNs                     â”‚      â”‚      â”‚
â”‚  â”‚    â”‚  â€¢ RNNs                     â”‚      â”‚      â”‚
â”‚  â”‚    â”‚  â€¢ Transformers             â”‚      â”‚      â”‚
â”‚  â”‚    â”‚  â€¢ GANs                     â”‚      â”‚      â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚      â”‚
â”‚  â”‚                                          â”‚      â”‚
â”‚  â”‚  â€¢ Decision Trees                        â”‚      â”‚
â”‚  â”‚  â€¢ Random Forests                        â”‚      â”‚
â”‚  â”‚  â€¢ SVM                                   â”‚      â”‚
â”‚  â”‚  â€¢ K-Means                               â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                      â”‚
â”‚  â€¢ Expert Systems                                    â”‚
â”‚  â€¢ Rule-based Systems                                â”‚
â”‚  â€¢ Search Algorithms (A*, Dijkstra)                  â”‚
â”‚  â€¢ Logic & Reasoning                                 â”‚
â”‚  â€¢ Natural Language Understanding                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Explanation:**

1. **Artificial Intelligence (AI)** - Outermost circle

   - Broadest concept
   - Any technique enabling computers to mimic human intelligence
   - Includes rule-based systems, expert systems, search algorithms
   - Example: Chess playing program with hardcoded rules

2. **Machine Learning (ML)** - Middle circle (subset of AI)

   - Systems that learn from data without explicit programming
   - Uses statistical techniques to find patterns
   - Includes traditional ML algorithms
   - Example: Spam filter learning from email data

3. **Deep Learning (DL)** - Inner circle (subset of ML)
   - Uses artificial neural networks with multiple layers
   - Automatically learns hierarchical features
   - Requires large amounts of data
   - Example: Image recognition using CNNs

**Hierarchy:**

```
DL âŠ‚ ML âŠ‚ AI

Every DL model is an ML model
Every ML model is an AI system
But not every AI uses ML
And not every ML uses DL
```

---

## Question 4

**Write the equation for gradient descent rule.**

### Solution:

#### **Gradient Descent Update Rule:**

```
w_new = w_old - Î± Â· âˆ‡L(w)
```

Or in expanded form:

```
w_(t+1) = w_t - Î± Â· (âˆ‚L/âˆ‚w)|_(w=w_t)
```

**Where:**

- **w_new** = updated weight (parameter) at iteration t+1
- **w_old** (or w_t) = current weight at iteration t
- **Î±** (or Î·) = learning rate (step size, typically 0.001 to 0.1)
- **âˆ‡L(w)** = gradient of loss function with respect to weight w
- **âˆ‚L/âˆ‚w** = partial derivative of loss with respect to weight
- **t** = current iteration number

#### **For Multiple Parameters:**

```
Î¸_(t+1) = Î¸_t - Î± Â· âˆ‡_Î¸ L(Î¸_t)
```

Or component-wise:

```
wâ‚_(t+1) = wâ‚_t - Î± Â· (âˆ‚L/âˆ‚wâ‚)
wâ‚‚_(t+1) = wâ‚‚_t - Î± Â· (âˆ‚L/âˆ‚wâ‚‚)
   â‹®
wâ‚™_(t+1) = wâ‚™_t - Î± Â· (âˆ‚L/âˆ‚wâ‚™)
```

#### **Vector Form:**

```
Î¸_new = Î¸_old - Î± Â· âˆ‡J(Î¸)

Where Î¸ = [wâ‚, wâ‚‚, ..., wâ‚™, b]áµ€ (all parameters)
```

#### **Interpretation:**

- **Negative sign (-)**: Move in the opposite direction of the gradient (downhill)
- **Gradient (âˆ‡L)**: Points in the direction of steepest ascent
- **Learning rate (Î±)**: Controls how big each step is
  - Too large: might overshoot the minimum
  - Too small: slow convergence

#### **Example:**

Given loss function L = (w - 3)Â²:

```
âˆ‚L/âˆ‚w = 2(w - 3)

If w_old = 5, Î± = 0.1:
w_new = 5 - 0.1 Â· 2(5 - 3)
w_new = 5 - 0.1 Â· 4
w_new = 5 - 0.4 = 4.6

(Moving closer to minimum at w = 3)
```

---

## Question 5

**Increasing the number of neurons helps to:**

- (a) Increase network depth
- (b) Shift the activation function
- (c) Reduce overfitting
- (d) Speed up training

### Solution:

**Answer: (a) Increase network depth**

**Explanation:**

Adding more neurons to a network increases its **capacity** and **representational power**, which relates to network depth and complexity.

**Detailed Analysis:**

**(a) Increase network depth** âœ“

- While adding neurons to existing layers increases **width**, adding more layers with neurons increases **depth**
- More neurons generally means the network can learn more complex patterns and representations
- This is the most accurate answer in the context of increasing model capacity

**(b) Shift the activation function** âœ—

- Adding neurons does NOT shift the activation function
- The activation function (ReLU, sigmoid, tanh) is chosen independently
- Each neuron uses the same activation function type

**(c) Reduce overfitting** âœ—

- Actually, increasing neurons typically **increases** overfitting risk
- More neurons = more parameters = higher capacity to memorize training data
- Regularization techniques (dropout, L2) are needed to combat this

**(d) Speed up training** âœ—

- More neurons means MORE computations per forward/backward pass
- Training becomes **slower**, not faster
- More parameters to update means longer training time

**Clarification:**

The question is somewhat ambiguous:

- Adding neurons to a **layer** increases **width**
- Adding new **layers** with neurons increases **depth**

In practice, increasing neurons:

- **Pros:** Better representation capacity, can learn more complex functions
- **Cons:** More computation, higher overfitting risk, slower training

**More accurate statement:** Increasing the number of neurons helps to **increase the model's capacity and representational power**, though the question's answer (a) is the best choice among the given options.

---

## Question 6

**Backpropagation is primarily used for:**

- (a) Weight initialization
- (b) Weight update through gradient computation
- (c) Activation selection
- (d) Data preprocessing

### Solution:

**Answer: (b) Weight update through gradient computation**

**Explanation:**

Backpropagation (backward propagation of errors) is the fundamental algorithm for **training neural networks** by computing gradients of the loss function with respect to all network weights.

**Why Option (b) is Correct:**

Backpropagation performs two key functions:

1. **Gradient Computation:**

   - Calculates âˆ‚L/âˆ‚w for every weight in the network
   - Uses the chain rule to propagate errors backward through layers
   - Example: âˆ‚L/âˆ‚wâ‚ = âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚z Â· âˆ‚z/âˆ‚wâ‚

2. **Enables Weight Updates:**
   - Provides gradients needed for optimization algorithms (SGD, Adam, etc.)
   - Update rule: w_new = w_old - Î± Â· (âˆ‚L/âˆ‚w)
   - Without backpropagation, we couldn't train deep networks efficiently

**Why Other Options are Wrong:**

**(a) Weight initialization** âœ—

- Weight initialization happens **before** training starts
- Common methods: Xavier, He initialization, random normal
- Backpropagation is not involved in initialization
- It only updates weights that are already initialized

**(c) Activation selection** âœ—

- Activation functions (ReLU, sigmoid, tanh) are chosen during **architecture design**
- This is a manual decision by the network designer
- Backpropagation uses the derivative of activations but doesn't select them

**(d) Data preprocessing** âœ—

- Preprocessing includes normalization, scaling, augmentation
- Happens **before** data enters the network
- Completely separate from backpropagation
- Example: StandardScaler, MinMaxScaler

**The Backpropagation Process:**

```
1. Forward Pass:
   Input â†’ Layer 1 â†’ Layer 2 â†’ ... â†’ Output â†’ Loss

2. Backward Pass (Backpropagation):
   Loss â†’ âˆ‚L/âˆ‚w_n â†’ ... â†’ âˆ‚L/âˆ‚w_2 â†’ âˆ‚L/âˆ‚w_1

3. Weight Update:
   w = w - Î± Â· âˆ‚L/âˆ‚w (using computed gradients)
```

**Key Principle:**

Backpropagation uses the **chain rule** to efficiently compute gradients:

```
âˆ‚L/âˆ‚wâ‚ = (âˆ‚L/âˆ‚y) Â· (âˆ‚y/âˆ‚z) Â· (âˆ‚z/âˆ‚wâ‚)
```

This allows training of deep networks with millions of parameters in reasonable time.

---

## Question 7

**Write the equation for the tanh(x) activation function.**

### Solution:

#### **Hyperbolic Tangent (tanh) Activation Function:**

```
tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```

#### **Alternative Forms:**

**1. Using Exponentials:**

```
tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
```

**2. Using Sigmoid Function:**

```
tanh(x) = 2Â·Ïƒ(2x) - 1

where Ïƒ(x) = 1/(1 + e^(-x)) is the sigmoid function

Therefore:
tanh(x) = 2/(1 + e^(-2x)) - 1
```

**3. Simplified:**

```
tanh(x) = (e^(2x) - 1) / (e^(2x) + 1)
```

#### **Properties:**

| Property          | Value                   |
| ----------------- | ----------------------- |
| **Range**         | (-1, 1)                 |
| **Domain**        | (-âˆ, âˆ)                 |
| **Zero-centered** | Yes (outputs around 0)  |
| **Derivative**    | tanh'(x) = 1 - tanhÂ²(x) |
| **At x=0**        | tanh(0) = 0             |
| **At xâ†’âˆ**        | tanh(âˆ) = 1             |
| **At xâ†’-âˆ**       | tanh(-âˆ) = -1           |

#### **Derivative:**

```
d/dx[tanh(x)] = 1 - tanhÂ²(x) = sechÂ²(x)
```

Or in terms of x:

```
tanh'(x) = 4e^(2x) / (e^(2x) + 1)Â²
```

#### **Key Values:**

```
tanh(0) = 0
tanh(1) â‰ˆ 0.762
tanh(2) â‰ˆ 0.964
tanh(5) â‰ˆ 0.9999
tanh(-1) â‰ˆ -0.762
```

#### **Graph Shape:**

```
        1 |     ________________
          |    /
          |   /
          |  /
    tanh  | /
          |/
    0 ----+-------------------- x
          |
          |
       -1 |________________

      S-shaped curve (sigmoid-like)
      Zero-centered (crosses at origin)
```

#### **Comparison with Sigmoid:**

| Feature       | tanh(x)                       | sigmoid(x)     |
| ------------- | ----------------------------- | -------------- |
| Range         | (-1, 1)                       | (0, 1)         |
| Zero-centered | Yes                           | No             |
| Formula       | (e^x - e^(-x))/(e^x + e^(-x)) | 1/(1 + e^(-x)) |
| Middle value  | 0                             | 0.5            |

#### **Usage in Neural Networks:**

- **Advantages over sigmoid:**
  - Zero-centered outputs (better gradient flow)
  - Stronger gradients (derivative range [0,1] vs sigmoid's [0,0.25])
- **Disadvantages:**

  - Still suffers from vanishing gradient problem for very large |x|
  - More expensive to compute than ReLU

- **Common use cases:**
  - RNN/LSTM cells (gate activations)
  - Hidden layers in shallow networks
  - When zero-centered outputs are beneficial

---

**END OF SOLUTIONS**

---

## Summary

This examination covered fundamental concepts in Deep Learning including:

- **Part A:**

  - Perceptron design and training (OR gate)
  - Classification vs Regression with loss functions
  - Forward pass and backpropagation in feedforward networks
  - Optimization algorithms (SGD vs Adam)

- **Part B:**
  - AI, ML, DL relationships
  - Gradient descent fundamentals
  - Neural network architecture concepts
  - Backpropagation purpose
  - Activation functions

**Key Takeaways:**

- Understanding of basic neural network operations
- Gradient-based optimization techniques
- Loss functions for different problem types
- Modern optimization improvements over classical methods

**Total Marks: 25**

Good luck with your examination! ğŸ“
