# DEEP LEARNING EXAM ANSWERS - SECTION B

**SRM University - AP | CSE424 End Term Exam**

---

## **Q11. LSTM for Medical Time Series Prediction (16 Marks)**

### **Part A: LSTM Architecture for Medical Data (8 Marks)**

#### **Importance of Time Series in Medical Field:**

- **Early Warning Systems:** Detect critical events (cardiac arrest, sepsis) before they occur
- **Continuous Monitoring:** Track patient vitals (heart rate, BP, temperature) in real-time
- **Pattern Recognition:** Identify abnormal trends that indicate deterioration
- **Improved Outcomes:** Timely intervention reduces mortality and improves recovery
- **Resource Optimization:** Predict ICU bed requirements, staff allocation

#### **LSTM Architecture for Medical Time Series:**

**Basic Structure:**

```
Input Layer â†’ LSTM Layers â†’ Dense Layers â†’ Output Layer

Input: [batch_size, timesteps, features]
Example: [32, 24, 5] = 32 patients, 24 hours, 5 vitals
```

**Architecture Components:**

1. **Input Layer:**

   - Multiple physiological parameters (heart rate, BP, temp, SpO2, respiratory rate)
   - Time steps: Sequential readings (e.g., hourly readings for 24 hours)

2. **LSTM Layers (Stacked):**

   - Layer 1: 128 LSTM units (capture short-term patterns)
   - Layer 2: 64 LSTM units (capture long-term dependencies)
   - Return sequences: True (for multi-layer stacking)

3. **Dense Layers:**

   - Fully connected layer: 32 neurons with ReLU
   - Output layer: 1 neuron with Sigmoid (binary: critical event yes/no)

4. **Dropout:** 0.2-0.3 between layers to prevent overfitting

**Visual:**

```
[HR, BP, Temp, SpO2, RR] (t-23)
           â†“
[HR, BP, Temp, SpO2, RR] (t-22)
           â†“
         ...
           â†“
[HR, BP, Temp, SpO2, RR] (t-1)
           â†“
[HR, BP, Temp, SpO2, RR] (t)
           â†“
    LSTM Cell (128) â†’ LSTM Cell (64)
           â†“
      Dense (32)
           â†“
    Output: P(critical event)
```

#### **Memory Cells & Gating Mechanism:**

**1. Memory Cell (Cell State):**

- Long-term memory storage
- Runs through entire sequence
- Stores important medical patterns (e.g., "BP declining for 6 hours")

**2. Three Gates:**

**Forget Gate (f_t):**
$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

- **Role:** Decide what to forget from cell state
- **Medical Example:** Discard old normal readings when new critical pattern emerges
- Output: 0 (forget completely) to 1 (keep completely)

**Input Gate (i_t):**
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

- **Role:** Decide what new information to store
- **Medical Example:** Store sudden BP spike or heart rate increase
- Combines: What to add (i_t) + Candidate values ($\tilde{C}_t$)

**Output Gate (o_t):**
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
$$h_t = o_t \cdot \tanh(C_t)$$

- **Role:** Decide what to output as hidden state
- **Medical Example:** Output critical warning when vital patterns indicate risk

**Cell State Update:**
$$C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$$

**Why This Matters for Medical Data:**

- **Long-term Dependencies:** Remember patient baseline from hours ago
- **Selective Memory:** Keep critical trends, forget noise
- **Gradient Flow:** Avoid vanishing gradient (learn from distant past)
- **Pattern Recognition:** Detect deterioration patterns (e.g., sepsis onset over 12-24 hours)

---

### **Part B: Preprocessing & Training (8 Marks)**

#### **Challenges in Medical Time Series:**

1. **Missing Data:**

   - Sensors malfunction, measurements skipped
   - **Solution:** Forward fill, backward fill, or interpolation

2. **Irregular Sampling:**

   - Vitals recorded at irregular intervals
   - **Solution:** Resample to fixed intervals (e.g., every 1 hour)

3. **Different Scales:**

   - Heart rate (60-100), BP (80-120), Temp (36-38Â°C)
   - **Solution:** Normalization required

4. **Class Imbalance:**

   - Critical events rare (e.g., 5% cardiac arrests)
   - **Solution:** SMOTE, class weights, focal loss

5. **Temporal Alignment:**
   - Different vitals measured at different times
   - **Solution:** Align to common timestamps

#### **Preprocessing Steps:**

**1. Data Cleaning:**

```python
# Remove outliers
heart_rate = clip(heart_rate, 30, 200)
bp_systolic = clip(bp_systolic, 50, 250)
```

**2. Handling Missing Values:**

```python
# Forward fill for short gaps (<2 hours)
df.fillna(method='ffill', limit=2)
# Linear interpolation for longer gaps
df.interpolate(method='linear')
```

**3. Normalization (Critical!):**

**Min-Max Scaling:**
$$x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}$$

**Z-Score Normalization (Better for medical data):**
$$x_{norm} = \frac{x - \mu}{\sigma}$$

```python
# Per-feature normalization
hr_norm = (hr - hr.mean()) / hr.std()
bp_norm = (bp - bp.mean()) / bp.std()
temp_norm = (temp - temp.mean()) / temp.std()
```

**4. Feature Engineering:**

Create derived features:

- **Rate of change:** Î”HR = HR(t) - HR(t-1)
- **Moving averages:** MA_6hr for smoothing
- **Variability:** Std deviation over last 3 hours
- **Time features:** Hour of day, day of week

**5. Sequence Creation:**

```python
# Create sliding windows
window_size = 24  # 24 hours of history
for i in range(len(data) - window_size):
    X.append(data[i:i+window_size])
    y.append(label[i+window_size])  # Predict next event
```

#### **Training Process:**

**1. Dataset Split:**

```
Train: 70% | Validation: 15% | Test: 15%
(Temporal split - no data leakage!)
```

**2. Model Configuration:**

```python
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(24, 5)),
    Dropout(0.3),
    LSTM(64, return_sequences=False),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
```

**3. Compilation:**

```python
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy', 'AUC', 'precision', 'recall']
)
```

**4. Training:**

```python
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32,
    class_weight={0: 1, 1: 10},  # Handle imbalance
    callbacks=[EarlyStopping(patience=10), ModelCheckpoint()]
)
```

#### **Evaluation Metrics (Why They Matter):**

**1. Accuracy:**

- **Not enough!** With 95% non-critical events, predicting "no event" gives 95% accuracy
- **Use:** Basic performance indicator

**2. Precision:**
$$\text{Precision} = \frac{TP}{TP + FP}$$

- **Medical Meaning:** Of all predicted critical events, how many were real?
- **Important:** Avoid alarm fatigue from false alarms

**3. Recall (Sensitivity):**
$$\text{Recall} = \frac{TP}{TP + FN}$$

- **Medical Meaning:** Of all real critical events, how many did we catch?
- **Critical!** Missing a cardiac arrest is life-threatening
- **Goal:** High recall (>90%) to catch all critical events

**4. F1-Score:**
$$F_1 = 2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

- **Balance** between precision and recall

**5. AUC-ROC:**

- **Best for imbalanced data**
- Measures model's ability to distinguish classes
- **Target:** AUC > 0.85

**6. Precision-Recall Curve:**

- Better than ROC for highly imbalanced medical data
- Focus on minority class (critical events)

**7. Time-to-Event:**

- How early does model predict critical event?
- **Goal:** 2-6 hours advance warning for intervention

**Performance Reflection:**

- **High Recall (>90%):** Model catches most critical events â†’ Lives saved
- **Moderate Precision (60-70%):** Some false alarms acceptable in medical setting
- **AUC > 0.85:** Good discrimination between stable and critical patients
- **Early Warning:** Predictions 4-6 hours before event â†’ Time for intervention

---

## **Q12. AlexNet & CNN Applications (16 Marks)**

### **Part A: AlexNet Architecture (8 Marks)**

#### **Overview:**

- **Year:** 2012 (ImageNet winner)
- **Revolutionized:** Deep learning in computer vision
- **Key Innovation:** First successful deep CNN (8 layers)
- **Performance:** 15.3% top-5 error (vs 26.2% previous best)

#### **Architecture Details:**

**Input:** 227Ã—227Ã—3 RGB images

**Layer-by-Layer Breakdown:**

**1. Conv1 + MaxPool1:**

```
Conv1: 96 filters, 11Ã—11, stride=4
Output: 55Ã—55Ã—96
Activation: ReLU
MaxPool: 3Ã—3, stride=2
Output: 27Ã—27Ã—96
```

- Large 11Ã—11 filters capture low-level features (edges, colors)
- Stride=4 reduces computation

**2. Conv2 + MaxPool2:**

```
Conv2: 256 filters, 5Ã—5, padding=2
Output: 27Ã—27Ã—256
Activation: ReLU
MaxPool: 3Ã—3, stride=2
Output: 13Ã—13Ã—256
```

- Smaller filters for mid-level features (textures, patterns)

**3. Conv3:**

```
Conv3: 384 filters, 3Ã—3, padding=1
Output: 13Ã—13Ã—384
Activation: ReLU
```

- No pooling, maintains spatial resolution

**4. Conv4:**

```
Conv4: 384 filters, 3Ã—3, padding=1
Output: 13Ã—13Ã—384
Activation: ReLU
```

**5. Conv5 + MaxPool5:**

```
Conv5: 256 filters, 3Ã—3, padding=1
Output: 13Ã—13Ã—256
Activation: ReLU
MaxPool: 3Ã—3, stride=2
Output: 6Ã—6Ã—256
```

**6. Fully Connected Layers:**

```
Flatten: 6Ã—6Ã—256 = 9,216
FC6: 4,096 neurons + ReLU + Dropout(0.5)
FC7: 4,096 neurons + ReLU + Dropout(0.5)
FC8: 1,000 neurons + Softmax (1000 ImageNet classes)
```

#### **Visual Architecture:**

```
Input (227Ã—227Ã—3)
    â†“
Conv1 (11Ã—11Ã—96, s=4) â†’ ReLU â†’ MaxPool â†’ 27Ã—27Ã—96
    â†“
Conv2 (5Ã—5Ã—256) â†’ ReLU â†’ MaxPool â†’ 13Ã—13Ã—256
    â†“
Conv3 (3Ã—3Ã—384) â†’ ReLU â†’ 13Ã—13Ã—384
    â†“
Conv4 (3Ã—3Ã—384) â†’ ReLU â†’ 13Ã—13Ã—384
    â†“
Conv5 (3Ã—3Ã—256) â†’ ReLU â†’ MaxPool â†’ 6Ã—6Ã—256
    â†“
Flatten (9,216)
    â†“
FC6 (4,096) â†’ ReLU â†’ Dropout
    â†“
FC7 (4,096) â†’ ReLU â†’ Dropout
    â†“
FC8 (1,000) â†’ Softmax
    â†“
Output (Class probabilities)
```

#### **Key Innovations:**

**1. ReLU Activation:**

- **Before:** Tanh, Sigmoid (slow, vanishing gradient)
- **AlexNet:** ReLU = max(0, x)
- **Benefit:** 6x faster training, no saturation

**2. Dropout (0.5):**

- Randomly drop 50% neurons during training
- **Prevents overfitting** in large networks
- Ensemble effect

**3. Data Augmentation:**

- Random crops: 227Ã—227 from 256Ã—256
- Horizontal flips
- Color jittering (PCA on RGB)
- **Effect:** Increased dataset size 2048x

**4. GPU Training:**

- Trained on 2 GTX 580 GPUs
- Parallelization across GPUs
- 5-6 days training time

**5. Overlapping Pooling:**

- Pool 3Ã—3 with stride=2 (overlap!)
- **Traditional:** Pool 2Ã—2, stride=2 (no overlap)
- **Benefit:** 0.4% better accuracy, harder to overfit

**6. Local Response Normalization (LRN):**

- Normalize activations across channels
- Mimics lateral inhibition in neurons
- (Later replaced by Batch Normalization)

#### **Parameters:**

```
Total: ~60 million parameters
Most: In FC layers (FC6 & FC7)
Conv layers: ~3.5 million
FC layers: ~56.5 million
```

---

### **Part B: CNN Real-World Application (8 Marks)**

#### **Application: Medical Image Diagnosis (X-Ray Analysis)**

**Problem Statement:**

- Detect pneumonia, tuberculosis, COVID-19 from chest X-rays
- Shortage of radiologists, especially in rural areas
- Need for fast, accurate diagnosis

#### **CNN Solution Architecture:**

**1. Data:**

- Chest X-ray images: 224Ã—224 grayscale (or RGB duplicated)
- Labels: Normal, Pneumonia, TB, COVID-19
- Dataset: 10,000+ labeled X-rays

**2. Model (Transfer Learning Approach):**

```
Pretrained CNN (ResNet-50 on ImageNet)
    â†“
Freeze early layers (feature extractors)
    â†“
Fine-tune last layers on medical data
    â†“
Custom classifier:
  - GlobalAvgPool
  - Dense(512, ReLU)
  - Dropout(0.5)
  - Dense(4, Softmax)  # 4 classes
```

**3. Preprocessing:**

- Resize to 224Ã—224
- Normalization: pixel values / 255
- Data augmentation:
  - Rotation (Â±15Â°)
  - Zoom (Â±10%)
  - Horizontal flip
  - Brightness adjustment

**4. Training:**

```python
optimizer = Adam(lr=0.0001)
loss = categorical_crossentropy
metrics = accuracy, precision, recall, AUC
epochs = 50
batch_size = 32
```

**5. Output:**

- Class probabilities: [P(Normal), P(Pneumonia), P(TB), P(COVID)]
- Confidence score
- **Grad-CAM heatmap:** Shows which lung regions influenced decision

#### **Advantages of CNNs over Traditional ML:**

**1. Automatic Feature Learning:**

**Traditional ML (e.g., SVM):**

- Manual feature engineering required:
  - Edge detection (Sobel, Canny)
  - Texture features (LBP, GLCM)
  - Shape descriptors (HOG)
  - Statistical features (mean, variance)
- **Problem:** Requires domain expertise, time-consuming, may miss patterns

**CNN:**

- Learns features automatically from raw pixels
- Layer 1: Edges, corners
- Layer 2: Textures, patterns
- Layer 3: Lung shapes, consolidations
- Layer 4+: Disease-specific patterns
- **Benefit:** No manual feature design, discovers hidden patterns

**2. Spatial Hierarchy:**

**Traditional ML:**

- Treats image as flat vector
- Loses spatial relationships
- Example: Random Forest on flattened 224Ã—224 = 50,176 features

**CNN:**

- Preserves spatial structure through convolutions
- Understands "lung texture" is in specific region
- Pooling maintains translation invariance
- **Benefit:** Contextual understanding

**3. Translation Invariance:**

**Traditional ML:**

- Requires exact alignment
- Pneumonia in left lung vs right lung = different features

**CNN:**

- Detects pneumonia regardless of location
- Convolutional filters scan entire image
- Pooling provides robustness to small shifts
- **Benefit:** Works on varied X-ray positions

**4. Handling High-Dimensional Data:**

**Traditional ML:**

- 224Ã—224 image = 50,176 features
- Curse of dimensionality
- Overfitting with limited data

**CNN:**

- Parameter sharing (same filter across image)
- 3Ã—3 filter applied everywhere vs unique weights
- Example: AlexNet has 60M parameters but processes 227Ã—227 images efficiently
- **Benefit:** Fewer parameters, less overfitting

**5. End-to-End Learning:**

**Traditional ML Pipeline:**

```
X-ray â†’ Preprocessing â†’ Feature Extraction â†’ Feature Selection â†’ Classifier
(Each step separate, error accumulation)
```

**CNN Pipeline:**

```
X-ray â†’ CNN â†’ Diagnosis
(Single optimized system)
```

- **Benefit:** Joint optimization, no error propagation

**6. Transfer Learning:**

**Traditional ML:**

- Start from scratch for each task
- Need large labeled dataset

**CNN:**

- Use ImageNet pretrained weights
- Fine-tune on small medical dataset (1,000 images sufficient)
- **Benefit:** Works with limited medical data

**7. Performance Comparison:**

| Metric            | Traditional ML (SVM + HOG) | CNN (ResNet-50) |
| ----------------- | -------------------------- | --------------- |
| **Accuracy**      | 78-82%                     | 92-95%          |
| **Sensitivity**   | 75%                        | 94%             |
| **Specificity**   | 80%                        | 93%             |
| **AUC**           | 0.83                       | 0.96            |
| **Training Time** | Hours (feature extraction) | Minutes (GPU)   |
| **Inference**     | 200ms/image                | 20ms/image      |

**8. Interpretability:**

**Traditional ML:**

- Feature importance clear (e.g., "top-left texture score = 0.8")
- But features are hand-crafted

**CNN:**

- Grad-CAM, Saliency maps show focus regions
- Example: Heatmap highlights consolidation in lower right lung
- **Benefit:** Clinician can verify model reasoning

**9. Robustness:**

**Traditional ML:**

- Sensitive to image quality, noise
- Different X-ray machines require recalibration

**CNN:**

- Learns robust features through augmentation
- Handles variations in contrast, brightness
- **Benefit:** Generalizes across hospitals/equipment

**Real-World Impact:**

- **Faster Diagnosis:** 10 seconds vs 10 minutes (human radiologist)
- **Accessibility:** Deploy in rural clinics without specialists
- **Consistency:** No fatigue, same performance 24/7
- **Screening:** Process thousands of X-rays for mass screening
- **Cost:** Reduce healthcare costs by early detection

---

## **Q13. Feedforward Neural Networks for Fraud Detection (16 Marks)**

### **Part A: Architecture for Fraud Detection (8 Marks)**

#### **Introduction to Feedforward Networks:**

**What is Feedforward NN?**

- Simplest neural network architecture
- Information flows one direction: Input â†’ Hidden â†’ Output
- No loops or cycles (unlike RNNs)
- Also called: Multi-Layer Perceptron (MLP)

**Why Suitable for Fraud Detection?**

1. **Tabular Data:** Fraud detection uses structured data (transaction amount, location, time, merchant)
2. **Fast Inference:** Real-time transaction approval (<100ms)
3. **Non-linear Patterns:** Can learn complex fraud patterns
4. **Scalable:** Handles millions of transactions
5. **Proven:** Used by PayPal, Stripe, Mastercard

#### **Fraud Detection Task:**

**Input Features (Example):**

- Transaction amount ($)
- Time of transaction (hour, day)
- Location (country, city)
- Merchant category
- Card type
- Distance from last transaction
- Time since last transaction
- Average transaction amount (last 30 days)
- Number of transactions (last 24 hours)
- Device fingerprint
- IP address
- **Total:** 20-50 features

**Output:**

- Binary classification: Fraud (1) or Legitimate (0)
- OR: Probability score [0, 1] for fraud risk

#### **Network Architecture:**

**Recommended Structure:**

```
Input Layer (40 features)
    â†“
Hidden Layer 1 (128 neurons, ReLU)
    â†“
Dropout (0.3)
    â†“
Hidden Layer 2 (64 neurons, ReLU)
    â†“
Dropout (0.3)
    â†“
Hidden Layer 3 (32 neurons, ReLU)
    â†“
Dropout (0.2)
    â†“
Output Layer (1 neuron, Sigmoid)
```

**Detailed Layer Specifications:**

**1. Input Layer:**

```
Size: 40 neurons (one per feature)
Example features:
  - transaction_amount (normalized)
  - hour_of_day (0-23)
  - is_weekend (0/1)
  - merchant_category_encoded (one-hot)
  - distance_km (from last transaction)
  - ...
```

**2. Hidden Layer 1:**

```
Neurons: 128
Activation: ReLU(x) = max(0, x)
Purpose: Learn basic patterns
  - High amount + foreign country â†’ suspicious
  - Late night + unusual merchant â†’ flag
Equation: h1 = ReLU(W1 Â· x + b1)
```

**3. Dropout Layer 1:**

```
Rate: 0.3 (drop 30% neurons during training)
Purpose: Prevent overfitting
Why: Fraud data is noisy, many edge cases
```

**4. Hidden Layer 2:**

```
Neurons: 64
Activation: ReLU
Purpose: Combine basic patterns into complex fraud signatures
  - Multiple small transactions + new location + night = fraud
Equation: h2 = ReLU(W2 Â· h1 + b2)
```

**5. Dropout Layer 2:**

```
Rate: 0.3
```

**6. Hidden Layer 3:**

```
Neurons: 32
Activation: ReLU
Purpose: Final refinement, decision boundaries
Equation: h3 = ReLU(W3 Â· h2 + b3)
```

**7. Dropout Layer 3:**

```
Rate: 0.2
```

**8. Output Layer:**

```
Neurons: 1
Activation: Sigmoid(x) = 1 / (1 + e^(-x))
Output: Probability of fraud P(fraud) âˆˆ [0, 1]
Example:
  - Output = 0.95 â†’ 95% fraud probability â†’ BLOCK
  - Output = 0.15 â†’ 15% fraud probability â†’ ALLOW
  - Output = 0.60 â†’ 60% fraud probability â†’ REQUEST VERIFICATION
```

#### **Why These Choices?**

**Number of Layers (3 hidden):**

- 1 layer: Too simple, can't learn complex patterns
- 2-3 layers: Optimal for tabular data (fraud detection)
- 5+ layers: Overkill, overfitting risk, slower inference

**Neuron Counts (128 â†’ 64 â†’ 32):**

- Pyramid structure: Wide â†’ Narrow
- Wide layers: Capture many feature combinations
- Narrow layers: Distill into decision
- Rule of thumb: Start with 2-3x input features

**ReLU Activation:**

- **Advantages:**
  - Fast computation
  - No vanishing gradient
  - Sparse activation (some neurons = 0)
- **Alternative:** Leaky ReLU for hidden layers (prevents dead neurons)

**Sigmoid Output:**

- **Why:** Binary classification needs probability
- **Alternatives:**
  - Softmax (if multi-class: fraud type A, B, C)
  - Linear (if regression: fraud amount)

**Dropout:**

- **Critical for fraud detection!**
- Fraud patterns change over time
- Prevents memorizing specific fraud cases
- Forces network to learn general patterns

#### **Forward Propagation Example:**

```
Transaction: [$500, 2AM, Foreign_Country, Gas_Station, ...]

Step 1: Input
x = [500, 2, 1, 0, ..., 5000] (40 features, normalized)

Step 2: Hidden Layer 1
z1 = W1 Â· x + b1  (128 values)
h1 = ReLU(z1)     (128 values, some zeroed)

Step 3: Dropout (training only)
h1_dropped = h1 * dropout_mask (70% kept)

Step 4: Hidden Layer 2
z2 = W2 Â· h1_dropped + b2
h2 = ReLU(z2)

Step 5: Dropout
h2_dropped = h2 * dropout_mask

Step 6: Hidden Layer 3
z3 = W3 Â· h2_dropped + b3
h3 = ReLU(z3)

Step 7: Output
z_out = W_out Â· h3 + b_out
output = Sigmoid(z_out)

Result: output = 0.92 â†’ 92% fraud probability â†’ BLOCK TRANSACTION
```

#### **Training Setup:**

```python
model = Sequential([
    Dense(128, activation='relu', input_shape=(40,)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy', 'precision', 'recall', 'AUC']
)
```

---

### **Part B: Challenges & Solutions (8 Marks)**

#### **Challenge 1: Class Imbalance**

**Problem:**

- Fraud: 0.1-1% of transactions
- Legitimate: 99-99.9% of transactions
- Example: 1 fraud in 1,000 transactions
- **Impact:** Model predicts "all legitimate" â†’ 99.9% accuracy but useless!

**Solutions:**

**1. Class Weights:**

```python
# Give fraud class higher importance
class_weight = {0: 1, 1: 100}  # Fraud loss Ã— 100
model.fit(X, y, class_weight=class_weight)
```

**2. SMOTE (Synthetic Minority Oversampling):**

```python
from imblearn.over_sampling import SMOTE
X_resampled, y_resampled = SMOTE().fit_resample(X, y)
# Creates synthetic fraud examples
# Balances dataset to 50-50 or 30-70
```

**3. Undersampling:**

- Randomly remove legitimate transactions
- **Risk:** Lose information
- **Use:** Combined with oversampling

**4. Focal Loss:**
$$FL(p_t) = -\alpha_t (1-p_t)^\gamma \log(p_t)$$

- Focuses on hard examples (misclassified frauds)
- Down-weights easy examples

**5. Adjust Decision Threshold:**

```python
# Instead of threshold = 0.5
threshold = 0.3  # More sensitive to fraud
if prediction > 0.3:
    flag_as_fraud()
```

---

#### **Challenge 2: Real-Time Inference**

**Problem:**

- Transaction approval must be <100ms
- Network inference + feature extraction + database lookup
- High traffic: 10,000 TPS (transactions per second)

**Solutions:**

**1. Model Optimization:**

- **Pruning:** Remove unnecessary neurons (80% can be pruned)
- **Quantization:** Use INT8 instead of FLOAT32 (4x faster)
- **Knowledge Distillation:** Train small "student" model from large "teacher"

**2. Feature Caching:**

```python
# Pre-compute user statistics
user_profile = {
    'avg_transaction_30d': 150,
    'transaction_count_24h': 5,
    'usual_merchants': [...]
}
# Cache in Redis for instant lookup
```

**3. Asynchronous Processing:**

```
Transaction arrives
  â†“
Quick rules (50ms): Amount > $10,000? â†’ Flag
  â†“
If passes: Approve immediately
  â†“
Run NN in background (200ms)
  â†“
If fraud detected: Alert for manual review
```

**4. GPU Batch Processing:**

- Batch 100 transactions
- Process on GPU in 10ms
- Latency: 10ms per batch = 0.1ms per transaction

**5. Model Serving:**

```python
# Use TensorFlow Serving or ONNX Runtime
# Optimized inference engines
# Reduce latency from 100ms to 20ms
```

---

#### **Challenge 3: Concept Drift**

**Problem:**

- Fraud patterns change over time
- New fraud techniques emerge
- Seasonal patterns (holidays, Black Friday)
- Model trained on 2023 data fails in 2024

**Solutions:**

**1. Continuous Retraining:**

```python
# Weekly retraining schedule
every_week:
    new_data = get_transactions(last_7_days)
    model.fit(new_data, epochs=5)  # Fine-tune
    deploy(model)
```

**2. Online Learning:**

- Update model incrementally with each new fraud case
- **Algorithm:** Stochastic Gradient Descent (SGD)
- **Benefit:** Adapts in real-time

**3. Ensemble Models:**

```python
# Combine models trained on different time periods
prediction = 0.5 * model_2023.predict(X) +
             0.5 * model_2024.predict(X)
```

**4. Monitoring:**

```python
# Track model performance over time
if current_month_AUC < 0.85:
    trigger_retraining_alert()
```

**5. Feature Drift Detection:**

- Monitor feature distributions
- Alert if transaction_amount distribution changes
- Example: Mean shifts from $100 to $200 (inflation, new products)

---

#### **Challenge 4: Interpretability**

**Problem:**

- "Black box" model
- Regulators require explanations: "Why was this transaction blocked?"
- Customer service needs reasons
- Banks need transparency

**Solutions:**

**1. SHAP (SHapley Additive exPlanations):**

```python
import shap
explainer = shap.DeepExplainer(model, X_train)
shap_values = explainer.shap_values(X_test)

# For transaction blocked:
shap.force_plot(
    explainer.expected_value,
    shap_values[0],
    X_test[0]
)
# Output: "Blocked because:
#   - Transaction amount (+0.4 fraud score)
#   - Foreign country (+0.3)
#   - Late night (+0.2)"
```

**2. Feature Importance:**

```python
# Which features matter most?
importances = model.get_layer_weights()[0].abs().sum(axis=1)
# Top features: amount, location, time_since_last
```

**3. Rule Extraction:**

- Convert NN decisions to IF-THEN rules
- Example: "IF amount > $500 AND country = Nigeria THEN fraud"
- **Tool:** Decision tree approximation of NN

**4. Attention Mechanisms:**

- Add attention layer to highlight important features
- Visualize which features the network focused on

---

#### **Challenge 5: Data Privacy & Security**

**Problem:**

- Sensitive financial data
- GDPR, PCI-DSS compliance
- Can't store raw credit card numbers
- Cross-border data restrictions

**Solutions:**

**1. Tokenization:**

```python
# Don't store card number 4111-1111-1111-1111
# Store token: TKN_a7b3c9d2e5f8
# Features: card_token_hash (anonymized)
```

**2. Federated Learning:**

- Train on decentralized data
- Banks share model updates, not data
- **Benefit:** Privacy preserved, collective learning

**3. Differential Privacy:**

- Add noise to training data
- **Guarantee:** Individual transactions can't be identified
- **Trade-off:** Slight accuracy drop (1-2%)

**4. Encryption:**

- Encrypt data at rest and in transit
- Homomorphic encryption: Compute on encrypted data

---

#### **Challenge 6: Adversarial Attacks**

**Problem:**

- Fraudsters try to fool the model
- **Adversarial Example:** Slightly modify transaction to evade detection
- Example: Change $500 to $499.99 to avoid threshold

**Solutions:**

**1. Adversarial Training:**

```python
# Train on perturbed examples
for epoch in epochs:
    # Generate adversarial examples
    X_adv = X + epsilon * sign(gradient)
    # Train on both original and adversarial
    model.train_on_batch(X, y)
    model.train_on_batch(X_adv, y)
```

**2. Ensemble Diversity:**

- Train multiple models with different architectures
- Fraudster can't fool all at once

**3. Anomaly Detection Layer:**

- Add secondary model for "unusual but not in training data"
- Detect zero-day fraud techniques

**4. Human-in-the-Loop:**

- Flag high-risk transactions for manual review
- Learn from human decisions

---

#### **Summary: Complete Solution Pipeline**

```
Transaction Input
    â†“
Feature Engineering (10ms)
    â†“
Model Inference (20ms)
    â†“
Prediction: P(fraud) = 0.85
    â†“
Decision:
  - P < 0.3 â†’ Approve (low risk)
  - 0.3 â‰¤ P < 0.7 â†’ Request verification (medium risk)
  - P â‰¥ 0.7 â†’ Block (high risk)
    â†“
Log for Retraining
    â†“
SHAP Explanation (if blocked)
    â†“
Return to Customer/Bank
```

**Performance Metrics:**

- **Precision:** 85% (of blocked transactions, 85% are actual fraud)
- **Recall:** 92% (catch 92% of all frauds)
- **F1-Score:** 88.4%
- **AUC:** 0.94
- **Latency:** <50ms per transaction
- **False Positive Rate:** 0.5% (5 legitimate blocked per 1000)

---

**END OF FIRST 3 ANSWERS - Ready for next questions when you are! ðŸ“š**
